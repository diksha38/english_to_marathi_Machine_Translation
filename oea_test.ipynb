{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc-JDLFRy4n_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model,load_model, model_from_json\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import time\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/mar.txt','r') as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "id": "vGAkWdQfzDAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uncleaned_data_list = data.split('\\n')\n",
        "len(uncleaned_data_list)\n",
        "uncleaned_data_list = uncleaned_data_list[:38695]\n",
        "len(uncleaned_data_list)\n",
        "english_word = []\n",
        "marathi_word = []\n",
        "cleaned_data_list = []\n",
        "for word in uncleaned_data_list:\n",
        "  english_word.append(word.split('\\t')[:-1][0])\n",
        "  marathi_word.append(word.split('\\t')[:-1][1])\n",
        "language_data = pd.DataFrame(columns=['English','Marathi'])\n",
        "language_data['English'] = english_word\n",
        "language_data['Marathi'] = marathi_word\n",
        "language_data.to_csv('language_data.csv', index=False)"
      ],
      "metadata": {
        "id": "De4E5OhhzEPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language_data.shape\n",
        "language_data.head()\n",
        "language_data['English'].values\n",
        "language_data['Marathi'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcZfMMlIzGHY",
        "outputId": "81eec09b-fcb5-48b4-c9f8-a1ed24c39192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['जा.', 'पळ!', 'धाव!', ..., 'मला ऑस्ट्रेलियात राहायचं नाहीये.',\n",
              "       'मला स्वतःबद्दल बोलायचं नाहीये.',\n",
              "       'मी चुकून माझी हार्ड डिस्क पुसून टाकली.'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_text = language_data['English'].values\n",
        "marathi_text = language_data['Marathi'].values\n",
        "len(english_text), len(marathi_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tulJv3NwzGJx",
        "outputId": "66982148-712d-415a-d88f-91f197b3d714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38695, 38695)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to lower case\n",
        "english_text_ = [x.lower() for x in english_text]\n",
        "marathi_text_ = [x.lower() for x in marathi_text]\n",
        "\n",
        "english_text_ = [re.sub(\"'\",'',x) for x in english_text_]\n",
        "marathi_text_ = [re.sub(\"'\",'',x) for x in marathi_text_]\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuation = string.punctuation\n",
        "    cleaned_text = ''.join(char for char in text if char not in punctuation)\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_punctuation_from_list(text_list):\n",
        "    return [remove_punctuation(text) for text in text_list]\n",
        "\n",
        "# Applying the function to English and Marathi text\n",
        "english_text_ = remove_punctuation_from_list(english_text_)\n",
        "marathi_text_ = remove_punctuation_from_list(marathi_text_)\n",
        "\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "removed_digits_text = []\n",
        "for sent in english_text_:\n",
        "  sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n",
        "  removed_digits_text.append(' '.join(sentance))\n",
        "english_text_ = removed_digits_text\n",
        "\n",
        "# removing the digits from the marathi sentances\n",
        "marathi_text_ = [re.sub(\"[२३०८१५७९४६]\",\"\",x) for x in marathi_text_]\n",
        "marathi_text_ = [re.sub(\"[\\u200d]\",\"\",x) for x in marathi_text_]\n",
        "\n",
        "# removing the starting and ending whitespaces\n",
        "english_text_ = [x.strip() for x in english_text_]\n",
        "marathi_text_ = [x.strip() for x in marathi_text_]\n",
        "\n",
        "# Putting the start and end words in the marathi sentances\n",
        "marathi_text_ = [\"start \" + x + \" end\" for x in marathi_text_]\n",
        "# manipulated_marathi_text_\n",
        "marathi_text_[0], english_text_[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwUL9e_XzGMH",
        "outputId": "58844837-8ecd-4739-f784-cc28cb6053b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('start जा end', 'go')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sent(text):\n",
        "  '''\n",
        "  Take list on texts as input and\n",
        "  returns its tokenizer and enocoded text\n",
        "  '''\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(text)\n",
        "\n",
        "  return tokenizer, tokenizer.texts_to_sequences(text)\n",
        "  # Tokenize english and marathi sentences\n",
        "eng_tokenizer, eng_encoded= tokenize_sent(text= english_text_)\n",
        "mar_tokenizer, mar_encoded= tokenize_sent(text= marathi_text_)\n",
        "\n",
        "# English Word --> index dictionary\n",
        "eng_index_word = eng_tokenizer.index_word\n",
        "\n",
        "# English Index --> word dictionary\n",
        "eng_word_index= eng_tokenizer.word_index\n",
        "\n",
        "# size of English vocabulary for encoder input\n",
        "# For zero padding we have to add +1 in size\n",
        "ENG_VOCAB_SIZE = len(eng_tokenizer.word_counts)+1 #4494\n",
        "\n",
        "# Marathi Word --> index dict\n",
        "mar_word_index= mar_tokenizer.word_index\n",
        "\n",
        "# Marathi Index --> word dict\n",
        "mar_index_word = mar_tokenizer.index_word\n",
        "# marathi vocab size for decoder output\n",
        "MAR_VOCAB_SIZE=len(mar_tokenizer.word_counts)+1 #10642\n",
        "\n",
        "# Getting max length of English and Marathi sentences\n",
        "max_eng_len = 0\n",
        "for i in range(len(eng_encoded)):\n",
        "  if len(eng_encoded[i]) > max_eng_len:\n",
        "    max_eng_len= len(eng_encoded[i]) #9\n",
        "\n",
        "max_mar_len = 0\n",
        "for i in range(len(mar_encoded)):\n",
        "  if len(eng_encoded[i]) > max_mar_len:\n",
        "    max_mar_len= len(mar_encoded[i]) #9\n",
        "\n",
        "\n",
        "# Padding both\n",
        "eng_padded = pad_sequences(eng_encoded, maxlen=max_eng_len, padding='post')\n",
        "mar_padded = pad_sequences(mar_encoded, maxlen=max_mar_len, padding='post')\n",
        "\n",
        "# Convert to array\n",
        "eng_padded= np.array(eng_padded)\n",
        "mar_padded= np.array(mar_padded)\n",
        "\n",
        "# Split data into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(eng_padded, mar_padded, test_size=0.1, random_state=0)\n"
      ],
      "metadata": {
        "id": "l9GiTU9-zGQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 25\n",
        "# BUFFER_SIZE stores the number of training points\n",
        "BUFFER_SIZE = len(X_train)\n",
        "\n",
        "# BATCH_SIZE is set to 64. Training and gradient descent happens in batches of 64\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# the number of batches in one epoch (also, the number of steps during training, when we go batch by batch)\n",
        "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
        "\n",
        "# the length of the embedded vector\n",
        "embedding_dim = 256\n",
        "\n",
        "# no of GRUs\n",
        "units = 1024\n",
        "\n",
        "# Hidden dimension\n",
        "hidden_dim = 1024\n",
        "\n",
        "# now, we shuffle the dataset and split it into batches of 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # the remainder after splitting by 64 are dropped\n",
        "\n",
        "print(BUFFER_SIZE)\n",
        "print(BUFFER_SIZE//64)\n",
        "print(steps_per_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHyMktxVzGSd",
        "outputId": "53ade267-aeab-41bf-a522-f0293d139886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34825\n",
            "544\n",
            "544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Define the embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # Define the RNN layer, LSTM\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            hidden_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, input_sequence, states):\n",
        "        # Embed the input\n",
        "        embed = self.embedding(input_sequence)\n",
        "        # Call the LSTM unit\n",
        "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
        "\n",
        "        return output, state_h, state_c\n",
        "\n",
        "    def init_states(self, batch_size):\n",
        "        # Return a all 0s initial states\n",
        "        return (tf.zeros([batch_size, self.hidden_dim]),\n",
        "                tf.zeros([batch_size, self.hidden_dim]))\n"
      ],
      "metadata": {
        "id": "RaExRMDzzGUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Define the embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # Define the RNN layer, LSTM\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            hidden_dim, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, input_sequence, state):\n",
        "        # Embed the input\n",
        "        embed = self.embedding(input_sequence)\n",
        "        # Call the LSTM unit\n",
        "        lstm_out, state_h, state_c = self.lstm(embed, state)\n",
        "        # Dense layer to predict output token\n",
        "        logits = self.dense(lstm_out)\n",
        "\n",
        "        return logits, state_h, state_c"
      ],
      "metadata": {
        "id": "9u9FzNZOzGWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the encoder\n",
        "encoder = Encoder(ENG_VOCAB_SIZE, embedding_dim, hidden_dim)\n",
        "# Get the initial states\n",
        "initial_state = encoder.init_states(1)\n",
        "# Call the encoder for testing\n",
        "test_encoder_output = encoder(tf.constant(\n",
        "    [[1, 23, 4, 5, 0, 0]]), initial_state)\n",
        "print(test_encoder_output[0].shape)\n",
        "# Create the decoder\n",
        "decoder = Decoder(MAR_VOCAB_SIZE, embedding_dim, hidden_dim)\n",
        "# Get the initial states\n",
        "de_initial_state = test_encoder_output[1:]\n",
        "# Call the decoder for testing\n",
        "test_decoder_output = decoder(tf.constant(\n",
        "    [[1, 3, 5, 7, 9, 0, 0, 0]]), de_initial_state)\n",
        "print(test_decoder_output[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ3r8vlyzGY4",
        "outputId": "a73c8ac8-0dbf-4c45-f37a-8d9f6734188b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 6, 1024)\n",
            "(1, 8, 10642)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def loss_func(targets, logits):\n",
        "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True)\n",
        "    # Mask padding values, they do not have to compute for loss\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    # Calculate the loss value\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    # y_pred shape is batch_size, seq length, vocab size\n",
        "    # y_true shape is batch_size, seq length\n",
        "    pred_values = K.cast(K.argmax(y_pred, axis=-1), dtype='int32')\n",
        "    correct = K.cast(K.equal(y_true, pred_values), dtype='float32')\n",
        "\n",
        "    # 0 is padding, don't include those\n",
        "    mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
        "    n_correct = K.sum(mask * correct)\n",
        "    n_total = K.sum(mask)\n",
        "\n",
        "    return n_correct / n_total"
      ],
      "metadata": {
        "id": "fySaYIwuzGbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the @tf.function decorator to take advance of static graph computation\n",
        "@tf.function\n",
        "def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n",
        "    ''' A training step, train a batch of the data and return the loss value reached\n",
        "        Input:\n",
        "        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the input sequence\n",
        "        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the target seq, our target sequence\n",
        "        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the input sequence to the decoder, we use Teacher Forcing\n",
        "        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n",
        "            the initial state of the encoder\n",
        "        - optimizer: a tf.keras.optimizers.\n",
        "        Output:\n",
        "        - loss: loss value\n",
        "\n",
        "    '''\n",
        "    # Network’s computations need to be put under tf.GradientTape() to keep track of gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get the encoder outputs\n",
        "        en_outputs = encoder(input_seq, en_initial_states)\n",
        "        # Set the encoder and decoder states\n",
        "        en_states = en_outputs[1:]\n",
        "        de_states = en_states\n",
        "        # Get the encoder outputs\n",
        "        de_outputs = decoder(target_seq_in, de_states)\n",
        "        # Take the actual output\n",
        "        logits = de_outputs[0]\n",
        "        # Calculate the loss function\n",
        "        loss = loss_func(target_seq_out, logits)\n",
        "        acc = accuracy_fn(target_seq_out, logits)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    # Calculate the gradients for the variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    # Apply the gradients and update the optimizer\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss, acc"
      ],
      "metadata": {
        "id": "HLx6SOP3zP79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def main_train(encoder, decoder, dataset, n_epochs, batch_size, optimizer, checkpoint, checkpoint_prefix):\n",
        "    for epoch in range(n_epochs):\n",
        "        start = time.time()\n",
        "        en_initial_states = encoder.init_states(batch_size)\n",
        "        for batch, (input_seq, target_seq) in enumerate(dataset.take(-1)):\n",
        "            target_seq_in = target_seq[:, :-1]\n",
        "            target_seq_out = target_seq[:, 1:]\n",
        "            loss, accuracy = train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer)\n",
        "            if batch % 100 == 0:\n",
        "                print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, batch, loss.numpy(), accuracy.numpy()))\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "        print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, loss.numpy(), accuracy.numpy()))\n",
        "        print('Time taken for 1 epoch {:.2f} sec\\n'.format(time.time() - start))\n",
        "# Create an Adam optimizer and clips gradients by norm\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "# Create a checkpoint object to save the model\n",
        "checkpoint_dir = './training_ckpt_seq2seq'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "# Train the model\n",
        "main_train(encoder, decoder, dataset, EPOCHS, BATCH_SIZE, optimizer, checkpoint, checkpoint_prefix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs-swu4wzRZ9",
        "outputId": "ed8d8a30-221a-49a6-f0ad-f1d783ba2080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 5.9584 Accuracy 0.0000\n",
            "Epoch 1 Batch 100 Loss 3.8391 Accuracy 0.2324\n",
            "Epoch 1 Batch 200 Loss 3.3065 Accuracy 0.2378\n",
            "Epoch 1 Batch 300 Loss 3.3664 Accuracy 0.2380\n",
            "Epoch 1 Batch 400 Loss 3.1226 Accuracy 0.2606\n",
            "Epoch 1 Batch 500 Loss 3.2151 Accuracy 0.2786\n",
            "Epoch 1 Loss 2.9702 Accuracy 0.3114\n",
            "Time taken for 1 epoch 41.01 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.7680 Accuracy 0.2937\n",
            "Epoch 2 Batch 100 Loss 2.7630 Accuracy 0.3304\n",
            "Epoch 2 Batch 200 Loss 2.8169 Accuracy 0.3254\n",
            "Epoch 2 Batch 300 Loss 2.7491 Accuracy 0.3452\n",
            "Epoch 2 Batch 400 Loss 2.4665 Accuracy 0.3982\n",
            "Epoch 2 Batch 500 Loss 2.3268 Accuracy 0.3789\n",
            "Epoch 2 Loss 2.4136 Accuracy 0.3494\n",
            "Time taken for 1 epoch 19.90 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.9674 Accuracy 0.4107\n",
            "Epoch 3 Batch 100 Loss 2.2590 Accuracy 0.4077\n",
            "Epoch 3 Batch 200 Loss 2.1288 Accuracy 0.3994\n",
            "Epoch 3 Batch 300 Loss 1.9834 Accuracy 0.4192\n",
            "Epoch 3 Batch 400 Loss 1.9910 Accuracy 0.4252\n",
            "Epoch 3 Batch 500 Loss 1.8367 Accuracy 0.4728\n",
            "Epoch 3 Loss 1.9598 Accuracy 0.4331\n",
            "Time taken for 1 epoch 18.44 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5741 Accuracy 0.4650\n",
            "Epoch 4 Batch 100 Loss 1.5911 Accuracy 0.5286\n",
            "Epoch 4 Batch 200 Loss 1.4523 Accuracy 0.5499\n",
            "Epoch 4 Batch 300 Loss 1.4732 Accuracy 0.4734\n",
            "Epoch 4 Batch 400 Loss 1.4443 Accuracy 0.5361\n",
            "Epoch 4 Batch 500 Loss 1.2858 Accuracy 0.6069\n",
            "Epoch 4 Loss 1.4898 Accuracy 0.5507\n",
            "Time taken for 1 epoch 20.20 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.1866 Accuracy 0.5864\n",
            "Epoch 5 Batch 100 Loss 1.1740 Accuracy 0.5777\n",
            "Epoch 5 Batch 200 Loss 1.2420 Accuracy 0.6441\n",
            "Epoch 5 Batch 300 Loss 1.0725 Accuracy 0.6317\n",
            "Epoch 5 Batch 400 Loss 1.1510 Accuracy 0.6108\n",
            "Epoch 5 Batch 500 Loss 1.1702 Accuracy 0.6418\n",
            "Epoch 5 Loss 1.0266 Accuracy 0.6686\n",
            "Time taken for 1 epoch 19.13 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.8084 Accuracy 0.7188\n",
            "Epoch 6 Batch 100 Loss 0.8361 Accuracy 0.6944\n",
            "Epoch 6 Batch 200 Loss 0.8557 Accuracy 0.7062\n",
            "Epoch 6 Batch 300 Loss 0.9139 Accuracy 0.6281\n",
            "Epoch 6 Batch 400 Loss 0.8156 Accuracy 0.6769\n",
            "Epoch 6 Batch 500 Loss 0.7893 Accuracy 0.6991\n",
            "Epoch 6 Loss 0.8096 Accuracy 0.7009\n",
            "Time taken for 1 epoch 19.67 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.5950 Accuracy 0.7537\n",
            "Epoch 7 Batch 100 Loss 0.5900 Accuracy 0.7665\n",
            "Epoch 7 Batch 200 Loss 0.6039 Accuracy 0.7600\n",
            "Epoch 7 Batch 300 Loss 0.5997 Accuracy 0.7602\n",
            "Epoch 7 Batch 400 Loss 0.6412 Accuracy 0.7401\n",
            "Epoch 7 Batch 500 Loss 0.6874 Accuracy 0.7304\n",
            "Epoch 7 Loss 0.7627 Accuracy 0.7262\n",
            "Time taken for 1 epoch 19.06 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.4782 Accuracy 0.8092\n",
            "Epoch 8 Batch 100 Loss 0.4951 Accuracy 0.8177\n",
            "Epoch 8 Batch 200 Loss 0.5027 Accuracy 0.7899\n",
            "Epoch 8 Batch 300 Loss 0.5634 Accuracy 0.7595\n",
            "Epoch 8 Batch 400 Loss 0.4922 Accuracy 0.8000\n",
            "Epoch 8 Batch 500 Loss 0.5823 Accuracy 0.7676\n",
            "Epoch 8 Loss 0.4764 Accuracy 0.7904\n",
            "Time taken for 1 epoch 20.13 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.3485 Accuracy 0.8661\n",
            "Epoch 9 Batch 100 Loss 0.3019 Accuracy 0.8536\n",
            "Epoch 9 Batch 200 Loss 0.3904 Accuracy 0.8069\n",
            "Epoch 9 Batch 300 Loss 0.3933 Accuracy 0.8303\n",
            "Epoch 9 Batch 400 Loss 0.4132 Accuracy 0.8155\n",
            "Epoch 9 Batch 500 Loss 0.3874 Accuracy 0.8393\n",
            "Epoch 9 Loss 0.4521 Accuracy 0.8123\n",
            "Time taken for 1 epoch 19.43 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3337 Accuracy 0.8647\n",
            "Epoch 10 Batch 100 Loss 0.2920 Accuracy 0.8838\n",
            "Epoch 10 Batch 200 Loss 0.3124 Accuracy 0.8555\n",
            "Epoch 10 Batch 300 Loss 0.3625 Accuracy 0.8353\n",
            "Epoch 10 Batch 400 Loss 0.3168 Accuracy 0.8607\n",
            "Epoch 10 Batch 500 Loss 0.4183 Accuracy 0.8258\n",
            "Epoch 10 Loss 0.3554 Accuracy 0.8318\n",
            "Time taken for 1 epoch 20.32 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2503 Accuracy 0.8928\n",
            "Epoch 11 Batch 100 Loss 0.2830 Accuracy 0.8812\n",
            "Epoch 11 Batch 200 Loss 0.3037 Accuracy 0.8697\n",
            "Epoch 11 Batch 300 Loss 0.3059 Accuracy 0.8694\n",
            "Epoch 11 Batch 400 Loss 0.3131 Accuracy 0.8701\n",
            "Epoch 11 Batch 500 Loss 0.3320 Accuracy 0.8426\n",
            "Epoch 11 Loss 0.3217 Accuracy 0.8709\n",
            "Time taken for 1 epoch 19.44 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.1851 Accuracy 0.8950\n",
            "Epoch 12 Batch 100 Loss 0.2060 Accuracy 0.9015\n",
            "Epoch 12 Batch 200 Loss 0.2799 Accuracy 0.8571\n",
            "Epoch 12 Batch 300 Loss 0.2677 Accuracy 0.8739\n",
            "Epoch 12 Batch 400 Loss 0.3349 Accuracy 0.8614\n",
            "Epoch 12 Batch 500 Loss 0.3229 Accuracy 0.8584\n",
            "Epoch 12 Loss 0.3471 Accuracy 0.8497\n",
            "Time taken for 1 epoch 20.14 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2381 Accuracy 0.8935\n",
            "Epoch 13 Batch 100 Loss 0.2591 Accuracy 0.8795\n",
            "Epoch 13 Batch 200 Loss 0.2467 Accuracy 0.8817\n",
            "Epoch 13 Batch 300 Loss 0.2897 Accuracy 0.8674\n",
            "Epoch 13 Batch 400 Loss 0.2752 Accuracy 0.8798\n",
            "Epoch 13 Batch 500 Loss 0.2834 Accuracy 0.8576\n",
            "Epoch 13 Loss 0.2859 Accuracy 0.8663\n",
            "Time taken for 1 epoch 19.32 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1989 Accuracy 0.8916\n",
            "Epoch 14 Batch 100 Loss 0.1762 Accuracy 0.9083\n",
            "Epoch 14 Batch 200 Loss 0.1924 Accuracy 0.8944\n",
            "Epoch 14 Batch 300 Loss 0.2552 Accuracy 0.8550\n",
            "Epoch 14 Batch 400 Loss 0.2353 Accuracy 0.9011\n",
            "Epoch 14 Batch 500 Loss 0.2668 Accuracy 0.8638\n",
            "Epoch 14 Loss 0.2566 Accuracy 0.8838\n",
            "Time taken for 1 epoch 20.08 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.2160 Accuracy 0.8886\n",
            "Epoch 15 Batch 100 Loss 0.2028 Accuracy 0.9067\n",
            "Epoch 15 Batch 200 Loss 0.2152 Accuracy 0.8964\n",
            "Epoch 15 Batch 300 Loss 0.2548 Accuracy 0.8802\n",
            "Epoch 15 Batch 400 Loss 0.2381 Accuracy 0.9027\n",
            "Epoch 15 Batch 500 Loss 0.2515 Accuracy 0.8765\n",
            "Epoch 15 Loss 0.2358 Accuracy 0.8618\n",
            "Time taken for 1 epoch 19.42 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1642 Accuracy 0.9176\n",
            "Epoch 16 Batch 100 Loss 0.2114 Accuracy 0.8985\n",
            "Epoch 16 Batch 200 Loss 0.1930 Accuracy 0.8889\n",
            "Epoch 16 Batch 300 Loss 0.2335 Accuracy 0.8892\n",
            "Epoch 16 Batch 400 Loss 0.2108 Accuracy 0.8813\n",
            "Epoch 16 Batch 500 Loss 0.2682 Accuracy 0.8655\n",
            "Epoch 16 Loss 0.3512 Accuracy 0.8215\n",
            "Time taken for 1 epoch 21.21 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1550 Accuracy 0.9254\n",
            "Epoch 17 Batch 100 Loss 0.1637 Accuracy 0.9063\n",
            "Epoch 17 Batch 200 Loss 0.1332 Accuracy 0.9275\n",
            "Epoch 17 Batch 300 Loss 0.2217 Accuracy 0.8960\n",
            "Epoch 17 Batch 400 Loss 0.2028 Accuracy 0.8908\n",
            "Epoch 17 Batch 500 Loss 0.2871 Accuracy 0.8513\n",
            "Epoch 17 Loss 0.2339 Accuracy 0.8892\n",
            "Time taken for 1 epoch 19.41 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1685 Accuracy 0.9003\n",
            "Epoch 18 Batch 100 Loss 0.1592 Accuracy 0.9167\n",
            "Epoch 18 Batch 200 Loss 0.1782 Accuracy 0.8906\n",
            "Epoch 18 Batch 300 Loss 0.2170 Accuracy 0.8931\n",
            "Epoch 18 Batch 400 Loss 0.2211 Accuracy 0.8867\n",
            "Epoch 18 Batch 500 Loss 0.2614 Accuracy 0.8774\n",
            "Epoch 18 Loss 0.2565 Accuracy 0.8844\n",
            "Time taken for 1 epoch 20.22 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1591 Accuracy 0.9121\n",
            "Epoch 19 Batch 100 Loss 0.1069 Accuracy 0.9407\n",
            "Epoch 19 Batch 200 Loss 0.1769 Accuracy 0.9132\n",
            "Epoch 19 Batch 300 Loss 0.1875 Accuracy 0.9050\n",
            "Epoch 19 Batch 400 Loss 0.1967 Accuracy 0.8879\n",
            "Epoch 19 Batch 500 Loss 0.2242 Accuracy 0.8817\n",
            "Epoch 19 Loss 0.2141 Accuracy 0.8704\n",
            "Time taken for 1 epoch 19.58 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1489 Accuracy 0.9267\n",
            "Epoch 20 Batch 100 Loss 0.1525 Accuracy 0.9192\n",
            "Epoch 20 Batch 200 Loss 0.1894 Accuracy 0.9029\n",
            "Epoch 20 Batch 300 Loss 0.1049 Accuracy 0.9344\n",
            "Epoch 20 Batch 400 Loss 0.1958 Accuracy 0.8836\n",
            "Epoch 20 Batch 500 Loss 0.2509 Accuracy 0.8889\n",
            "Epoch 20 Loss 0.1963 Accuracy 0.8970\n",
            "Time taken for 1 epoch 20.31 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1327 Accuracy 0.9240\n",
            "Epoch 21 Batch 100 Loss 0.1286 Accuracy 0.9164\n",
            "Epoch 21 Batch 200 Loss 0.1872 Accuracy 0.8974\n",
            "Epoch 21 Batch 300 Loss 0.1489 Accuracy 0.9164\n",
            "Epoch 21 Batch 400 Loss 0.2073 Accuracy 0.8985\n",
            "Epoch 21 Batch 500 Loss 0.2267 Accuracy 0.8812\n",
            "Epoch 21 Loss 0.1370 Accuracy 0.9205\n",
            "Time taken for 1 epoch 19.46 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.1119 Accuracy 0.9228\n",
            "Epoch 22 Batch 100 Loss 0.1544 Accuracy 0.9172\n",
            "Epoch 22 Batch 200 Loss 0.1479 Accuracy 0.9263\n",
            "Epoch 22 Batch 300 Loss 0.1423 Accuracy 0.9088\n",
            "Epoch 22 Batch 400 Loss 0.1672 Accuracy 0.8943\n",
            "Epoch 22 Batch 500 Loss 0.1651 Accuracy 0.9096\n",
            "Epoch 22 Loss 0.2037 Accuracy 0.8929\n",
            "Time taken for 1 epoch 21.29 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.1296 Accuracy 0.9475\n",
            "Epoch 23 Batch 100 Loss 0.1957 Accuracy 0.8967\n",
            "Epoch 23 Batch 200 Loss 0.1342 Accuracy 0.9207\n",
            "Epoch 23 Batch 300 Loss 0.1751 Accuracy 0.9088\n",
            "Epoch 23 Batch 400 Loss 0.2295 Accuracy 0.8609\n",
            "Epoch 23 Batch 500 Loss 0.2127 Accuracy 0.8808\n",
            "Epoch 23 Loss 0.2287 Accuracy 0.8599\n",
            "Time taken for 1 epoch 19.46 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.1269 Accuracy 0.9277\n",
            "Epoch 24 Batch 100 Loss 0.1389 Accuracy 0.9179\n",
            "Epoch 24 Batch 200 Loss 0.1352 Accuracy 0.9135\n",
            "Epoch 24 Batch 300 Loss 0.1953 Accuracy 0.9017\n",
            "Epoch 24 Batch 400 Loss 0.1611 Accuracy 0.8902\n",
            "Epoch 24 Batch 500 Loss 0.1655 Accuracy 0.9174\n",
            "Epoch 24 Loss 0.1721 Accuracy 0.8843\n",
            "Time taken for 1 epoch 20.27 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1236 Accuracy 0.9244\n",
            "Epoch 25 Batch 100 Loss 0.1480 Accuracy 0.9231\n",
            "Epoch 25 Batch 200 Loss 0.1400 Accuracy 0.9102\n",
            "Epoch 25 Batch 300 Loss 0.1326 Accuracy 0.9099\n",
            "Epoch 25 Batch 400 Loss 0.1730 Accuracy 0.9000\n",
            "Epoch 25 Batch 500 Loss 0.2185 Accuracy 0.8647\n",
            "Epoch 25 Loss 0.2119 Accuracy 0.8916\n",
            "Time taken for 1 epoch 19.51 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_output(input_text, encoder, input_max_len, tokenizer_inputs, word_index_outputs, index_word_outputs):\n",
        "    # Tokenize the input sequence\n",
        "    input_seq = tokenizer_inputs.texts_to_sequences([input_text])\n",
        "    # Pad the sentence\n",
        "    input_seq = pad_sequences(input_seq, maxlen=input_max_len, padding='post')\n",
        "    # Set the encoder initial state\n",
        "    en_initial_states = encoder.init_states(1)\n",
        "    en_outputs = encoder(tf.constant(input_seq), en_initial_states)\n",
        "    # Create the decoder input, the sos token\n",
        "    de_input = tf.constant([[word_index_outputs['start']]])\n",
        "    # Set the decoder states to the encoder vector or encoder hidden state\n",
        "    de_state_h, de_state_c = en_outputs[1:]\n",
        "\n",
        "    out_words = []\n",
        "    while True:\n",
        "        # Decode and get the output probabilities\n",
        "        de_output, de_state_h, de_state_c = decoder(\n",
        "            de_input, (de_state_h, de_state_c))\n",
        "        # Select the word with the highest probability\n",
        "        de_input = tf.argmax(de_output, -1)\n",
        "        # Get the predicted word index\n",
        "        predicted_index = de_input.numpy()[0][0]\n",
        "        # Check if the predicted word is 'end' or if max length is reached\n",
        "        if index_word_outputs[predicted_index] == 'end' or len(out_words) >= 20:\n",
        "            break\n",
        "        # Append the word to the predicted output\n",
        "        out_words.append(index_word_outputs[predicted_index])\n",
        "\n",
        "    return ' '.join(out_words)\n",
        "\n",
        "# Example input sentence\n",
        "input_sentence = \"i want to go to my home\"\n",
        "\n",
        "# Predict the output sequence\n",
        "output_sequence = predict_output(input_sentence, encoder, max_eng_len, eng_tokenizer, mar_word_index, mar_index_word)\n",
        "\n",
        "# Print the predicted output sequence\n",
        "print(\"Input Sentence:\", input_sentence)\n",
        "print(\"Predicted Output Sequence:\", output_sequence)\n"
      ],
      "metadata": {
        "id": "gG-JYx27zUHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d034dfb-3cbc-4bfc-8912-05e40a514cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence: i want to go to my home\n",
            "Predicted Output Sequence: मला माझ्या घरी जायचं आहे\n"
          ]
        }
      ]
    }
  ]
}